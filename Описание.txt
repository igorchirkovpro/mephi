Что сработало:

1.Замена обычного LSTM на двунаправленный GRU. Это соответствует тому, что в процессе определения ролей элементов предложения требуется учитывать не только информацию о предыдущих элементах, но и информацию о будущих. В целом, при использовании рекуррентных нейросетей для анализа текстов обычно используют именно двунаправленные рекуррентные слои.
2.Переход от обучения по единичным примерам к обучению по батчам - таким образом, мы позволяем нейросети видеть одновременно разнообразные примеры отношений между элементами предложений.
3.Увеличение количества рекуррентных слоёв с одного до трёх. Дело в том, что при одиночном проходе по последовательности мы получаем недостаточно абстрактные признаки - больше слоёв позволяют выявить больше внутренних взаимосвязей. Плюс, таким образом нейросеть обладает большей "памятью" за счёт многократной агрегации признаков.
4.Изменение размерности эмбеддингов так, чтобы сначала происходил переход от более низкоразмерных эмбеддингов в более высокоразмерные, а потом происходило упрощение до выходного размера. Таким образом, если смотреть геометрически, нейросеть может выявлять немонотонные взаимосвязи между признаками гораздо лучше.
5.Добавление dropout в рекуррентные слои - исключает слишком специфичные для тренировочной выборки абстрактные признаки, которые могла бы выявить нейросеть.
6.Замена оптимизатора на Adamax с L2 регуляризацией (для большей стабилизации весов рекуррентных слоёв) и достаточно высоким learning rate (0.01).
7.Установка scale_grad_by_freq=True в слое эмбеддингов. Таким образом, мы более равномерно корректируем наши эмбеддинги, и более редкие слова обновляются не менее сильно, чем более частые. Позволяет обучаться более равномерно.
8.Раньше делался model.zero_grad(), в то время, как для очистки градиентов нужно делать optimizer.zero_grad() - раньше градиенты накапливались, что ухудшало обучение.
9.Убрал log_softmax с конца нейросети и заменил функцию ошибки на CrossEntropyLoss - так повысится численная стабильность.