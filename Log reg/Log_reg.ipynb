{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data-logistic.csv', header=None)\n",
    "y = df[0]\n",
    "X = df.loc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выше выписаны правильные формулы для градиентного спуска. Мы используем полноценный градиентный спуск, а не его стохастический вариант\n",
    "def fw1(w1, w2, y, X, k, C):\n",
    "    l = len(y)\n",
    "    S = 0\n",
    "    for i in range(0, l):\n",
    "        S += y[i] * X[1][i] * (1.0 - 1.0 / (1.0 + math.exp(-y[i] * (w1*X[1][i] + w2*X[2][i]))))\n",
    "\n",
    "    return w1 + (k * (1.0 / l) * S) - k * C * w1\n",
    "\n",
    "def fw2(w1, w2, y, X, k, C):\n",
    "    l = len(y)\n",
    "    S = 0\n",
    "    for i in range(0, l):\n",
    "        S += y[i] * X[2][i] * (1.0 - 1.0 / (1.0 + math.exp(-y[i] * (w1*X[1][i] + w2*X[2][i]))))\n",
    "\n",
    "    return w2 + (k * (1.0 / l) * S) - k * C * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем градиентный спуск для обычной и L2-регуляризованной (с коэффициентом регуляризации 10) логистической регрессии. Используем длину шага k=0.1. В качестве начального приближения используем вектор (0, 0).\n",
    "def grad(y, X, C=0.0, w1=0.0, w2=0.0, k=0.1, err=1e-5):\n",
    "    i = 0\n",
    "    i_max = 10000\n",
    "    w1_new, w2_new = w1, w2\n",
    "\n",
    "    while True:\n",
    "        i += 1\n",
    "        w1_new, w2_new = fw1(w1, w2, y, X, k, C), fw2(w1, w2, y, X, k, C)\n",
    "        e = math.sqrt((w1_new - w1) ** 2 + (w2_new - w2) ** 2)\n",
    "        if i >= i_max or e <= err:\n",
    "            break\n",
    "        else:\n",
    "            w1, w2 = w1_new, w2_new\n",
    "\n",
    "    return [w1_new, w2_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запустим градиентный спуск и доведём до сходимости (евклидово расстояние между векторами весов на соседних итерациях должно быть не больше 1e-5). Ограничим сверху число итераций десятью тысячами.\n",
    "w1, w2 = grad(y, X)\n",
    "rw1, rw2 = grad(y, X, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.927 0.936\n"
     ]
    }
   ],
   "source": [
    "# На вход функции roc_auc_score нужно подавать оценки вероятностей, подсчитаем обученным алгоритмом. Для этого воспользуемся сигмоидной функцией: a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)).\n",
    "def a(X, w1, w2):\n",
    "    return 1.0 / (1.0 + math.exp(-w1 * X[1] - w2 * X[2]))\n",
    "\n",
    "y_score = X.apply(lambda x: a(x, w1, w2), axis=1)\n",
    "y_rscore = X.apply(lambda x: a(x, rw1, rw2), axis=1)\n",
    "\n",
    "auc = roc_auc_score(y, y_score)\n",
    "rauc = roc_auc_score(y, y_rscore)\n",
    "\n",
    "print (1, \"{:0.3f} {:0.3f}\".format(auc, rauc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
